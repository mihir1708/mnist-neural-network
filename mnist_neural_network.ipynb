{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **Importing all required libraries**"
      ],
      "metadata": {
        "id": "TaCeIFciiRSV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras import layers, models, optimizers\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "I2nDPAlwLLOU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Loading MNIST dataset, reshape and one-hot encode labels**"
      ],
      "metadata": {
        "id": "Lxk_FZEQihcL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EZ2OaQgMb9SO",
        "outputId": "9f114306-06df-492e-8686-1f87ea79b29c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train shape (60000, 28, 28) X_test shape (10000, 28, 28)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "print('X_train shape', X_train.shape, 'X_test shape', X_test.shape)\n",
        "\n",
        "# Reshape to (N, 28, 28, 1) so Conv2D expects the correct input shape\n",
        "X_train = X_train.reshape((X_train.shape[0], 28, 28, 1))\n",
        "X_test = X_test.reshape((X_test.shape[0], 28, 28, 1))\n",
        "\n",
        "# Convert integer labels to one-hot encoded vectors\n",
        "y_train_encoded = to_categorical(y_train)\n",
        "y_test_encoded = to_categorical(y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Displaying random samples from the training set**"
      ],
      "metadata": {
        "id": "m8f8Y_-EjNr7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "plt.figure(figsize = (12,5))\n",
        "for i in range(8):\n",
        "  ind = random.randint(0, len(X_train))\n",
        "  plt.subplot(240+1+i)\n",
        "  plt.imshow(X_train[ind, :, :, 0], cmap='gray')\n",
        "  plt.axis('off')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 290
        },
        "id": "QdGOcbRaHUal",
        "outputId": "74cae8ac-cff0-4e47-9a20-4ee89e8d1f99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x500 with 8 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Defining the CNN model**"
      ],
      "metadata": {
        "id": "vPr3VPZWjZl5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Architecture: Conv2D (ReLU), MaxPooling2D, Flatten, Dense(10, softmax)\n",
        "def build_model(num_filters=16, learning_rate=0.001):\n",
        "  model = models.Sequential()\n",
        "  #define filters and convolutional layers here\n",
        "  model.add(layers.Conv2D(filters=num_filters, kernel_size=(3, 3),\n",
        "  activation='relu', input_shape=(28, 28, 1)))\n",
        "  #Add a maxpooling layer\n",
        "  model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "  #Flatten the output and give it to a fully connected layer\n",
        "  model.add(layers.Flatten())\n",
        "  #one hidden layer maps the flattened neurons to output\n",
        "  model.add(layers.Dense(10, activation ='softmax'))\n",
        "  #compile with Adam optimizer and categorical cross-entropy loss\n",
        "  model.compile(optimizer=optimizers.Adam(learning_rate=learning_rate), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "  return model"
      ],
      "metadata": {
        "id": "uzL_83O9dhPP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Perform 5-fold stratified cross-validation**"
      ],
      "metadata": {
        "id": "zVDncmGKj9Tc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test different filter sizes and learning rates and then pick best one based on mean validation accuracy\n",
        "filters_list = [16, 32]\n",
        "lr_list = [0.001, 0.01]\n",
        "\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "best_cfg = None\n",
        "best_score = -1.0\n",
        "\n",
        "epochs = 3\n",
        "batch_size = 128\n",
        "\n",
        "for num_filters in filters_list:\n",
        "    for lr in lr_list:\n",
        "        scores = []\n",
        "        print(f\"\\nConfig: filters={num_filters}, lr={lr}\")\n",
        "        for fold_idx, (train_ix, val_ix) in enumerate(skf.split(X_train, y_train), start=1):\n",
        "            # Split training data for this fold\n",
        "            X_tr, X_val = X_train[train_ix], X_train[val_ix]\n",
        "            y_tr, y_val = y_train_encoded[train_ix], y_train_encoded[val_ix]\n",
        "            # Build, train, and evaluate model\n",
        "            model = build_model(num_filters=num_filters, learning_rate=lr)\n",
        "            model.fit(X_tr, y_tr, epochs=3, batch_size=128,validation_data=(X_val, y_val), verbose=0)\n",
        "            _, acc = model.evaluate(X_val, y_val, verbose=0)\n",
        "            scores.append(acc)\n",
        "            print(f\"  Fold {fold_idx}: val_acc={acc:.4f}\")\n",
        "        # Calculate mean and std validation accuracy\n",
        "        mean_acc = float(np.mean(scores))\n",
        "        std_acc  = float(np.std(scores))\n",
        "        print(f\"Mean val_acc: {mean_acc:.4f} (+/- {std_acc:.4f})\")\n",
        "        if mean_acc > best_score:\n",
        "            best_score = mean_acc\n",
        "            best_cfg = (num_filters, lr)\n",
        "\n",
        "print(f\"\\nBest config: filters={best_cfg[0]}, lr={best_cfg[1]} (mean val_acc={best_score:.4f})\")\n"
      ],
      "metadata": {
        "id": "1xv5JGmXd7y1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "837a7bd0-6a56-46c3-c826-17d35df02709"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Config: filters=16, lr=0.001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 1: val_acc=0.9646\n",
            "  Fold 2: val_acc=0.9614\n",
            "  Fold 3: val_acc=0.9709\n",
            "  Fold 4: val_acc=0.9668\n",
            "  Fold 5: val_acc=0.9648\n",
            "Mean val_acc: 0.9657 (+/- 0.0031)\n",
            "\n",
            "Config: filters=16, lr=0.01\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 1: val_acc=0.9488\n",
            "  Fold 2: val_acc=0.9474\n",
            "  Fold 3: val_acc=0.9518\n",
            "  Fold 4: val_acc=0.9486\n",
            "  Fold 5: val_acc=0.9417\n",
            "Mean val_acc: 0.9476 (+/- 0.0033)\n",
            "\n",
            "Config: filters=32, lr=0.001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 1: val_acc=0.9725\n",
            "  Fold 2: val_acc=0.9685\n",
            "  Fold 3: val_acc=0.9722\n",
            "  Fold 4: val_acc=0.9713\n",
            "  Fold 5: val_acc=0.9704\n",
            "Mean val_acc: 0.9710 (+/- 0.0014)\n",
            "\n",
            "Config: filters=32, lr=0.01\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fold 1: val_acc=0.9524\n",
            "  Fold 2: val_acc=0.9506\n",
            "  Fold 3: val_acc=0.9515\n",
            "  Fold 4: val_acc=0.9459\n",
            "  Fold 5: val_acc=0.9429\n",
            "Mean val_acc: 0.9487 (+/- 0.0036)\n",
            "\n",
            "Best config: filters=32, lr=0.001 (mean val_acc=0.9710)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Retrain best configuration on full training set**"
      ],
      "metadata": {
        "id": "RP_EznS0mRN3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Rebuild model with the best hyperparameters\n",
        "best_model = build_model(num_filters=best_cfg[0], learning_rate=best_cfg[1])\n",
        "# Train on the full training set\n",
        "best_model.fit(X_train, y_train_encoded, epochs=5, batch_size=128, verbose=1)\n",
        "# Evaluate performance on the held-out test set\n",
        "test_loss, test_acc = best_model.evaluate(X_test, y_test_encoded, verbose=0)\n",
        "print(f\"\\nFinal Test Accuracy: {test_acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wWKB3PVocZ7c",
        "outputId": "2d29ae34-f875-4e35-d28b-d62575d06ad2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 25ms/step - accuracy: 0.8579 - loss: 2.8035\n",
            "Epoch 2/5\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 25ms/step - accuracy: 0.9719 - loss: 0.1360\n",
            "Epoch 3/5\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 24ms/step - accuracy: 0.9824 - loss: 0.0667\n",
            "Epoch 4/5\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 24ms/step - accuracy: 0.9854 - loss: 0.0480\n",
            "Epoch 5/5\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 25ms/step - accuracy: 0.9907 - loss: 0.0287\n",
            "\n",
            "Final Test Accuracy: 0.9754\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Final Report**\n",
        "\n",
        "## **CNN Architecture**\n",
        "This lab's CNN is a compact model inspired by LeNet architecture designed for 28×28×1 grayscale MNIST digit images. It begins with a single Conv2D layer with a 3×3 kernel and ReLU activation. This layer introduces nonlinearity and captures local edges and curves. One of the most critical hyperparameters, the number of filters for this layer, is set to either 16 or 32. Next is a 2×2 MaxPooling layer which halves the spatial dimensions and provides computational efficiency and helps to introduce translation invariance. The output of the pooling layer is flattened to a 1-D vector and sent to a Dense layer with 10 output neurons for the 10 digit classes with a softmax activation, which provides the predicted probabilities for each class. The network is trained with the Adam optimizer, is set to measure accuracy, and the loss is categorical cross-entropy. The design is lightweight and passes the MNIST classification task reasonably well, as it captures the necessary spatial patterns with simple computational resources.\n",
        "\n",
        "## **Parameter Exploration Procedure**\n",
        "\n",
        "We used stratified 5-fold cross-validation to assess the model on the training data and determine the optimal set of hyperparameters while preserving class balance in each fold. For each parameter set, a model was trained for 3 epochs on a batch size of 128 samples per batch, and the validation accuracy for the held-out fold was obtained. Hyperparameters considered were the number of filters in the convolutional layer (16 and 32) and the learning rate for the Adam optimizer (0.001 and 0.01). For each configuration, the validation accuracy across the folds was averaged, and the one with the highest mean accuracy was selected. The test set was not used during cross validation.\n",
        "\n",
        "## **Results and Final Evaluation**\n",
        "While performing 5-fold stratified cross validation, four combinations of hyperparameters were analyzed to identify the best performing model. Setting the hyperparameters to 16 filters and a 0.001 learning rate, the model produced a mean validation accuracy of 0.9657 ± 0.0031. When the learning rate was raised to 0.01, the mean accuracy dropped slightly to 0.9476 ± 0.0033. Using 32 filters further increased mean validation accuracy to 0.9710 ± 0.0014 for a learning rate of 0.001, while a learning rate of 0.01 again reduced the mean to 0.9487 ± 0.0036. As a result, the configuration with 32 filters and a learning rate of 0.001 was chosen. It was then retrained from scratch on the entire 60,000-image training set for 5 epochs with a batch size of 128, yielding a final test accuracy of 0.9754 on the 10,000-image test set. These results indicate that the selected configuration was consistent with the results obtained during cross validation and generalised well to unseen data.\n",
        "\n",
        "## **Comparison with Other Papers on MNIST**\n",
        "\n",
        "When using more complex architectures and fine-tuned hyperparameters, more recent studies using Convolutional Neural Networks on the MNIST data set accomplishes test accuracies in the range of 99% and even higher. One CNN with hyperparameter tuning claims to have achieved a 99.40% test accuracy [1]. Another CNN that combines CNN features with classical methods claims to have achieved a 99.30% accuracy on MNIST [2]. Unlike these models, our lightweight, single-convolution model (which had no data normalization or augmentation) achieved 97.54% test accuracy after 5 epochs. This was lower than these stronger baselines, but it is expected given the less complex model and more limited training conditions. These references contextualize our performance relative to commonly reported MNIST baselines [1][2].\n",
        "\n",
        "## **References**\n",
        "\n",
        "[1] H. Shao, E. Ma, M. Zhu, X. Deng, and S. Zhai, “MNIST Handwritten Digit Classification Based on Convolutional Neural Network with Hyperparameter Optimization,” Intelligent Automation & Soft Computing, vol. 36, no. 3, pp. 3595–3606, 2023.\n",
        "\n",
        "[2] S. S. Ullah, Li Gang, M. Riaz, A. Ashfaq, S. Khan, and S. Khan, “Handwritten Digit Recognition: An Ensemble-Based Approach for Superior Performance,” arXiv preprint arXiv:2503.06104, 2025.\n",
        "\n",
        "ECE 449 Lab 3 PDF and Presentation\n",
        "\n",
        "https://lekhuyen.medium.com/lenet-and-mnist-handwritten-digit-classification-354f5646c590\n"
      ],
      "metadata": {
        "id": "jLn4cpKFpJWG"
      }
    }
  ]
}